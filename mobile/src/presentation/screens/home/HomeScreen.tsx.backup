/**
 * Home Screen
 * Main screen with listen button and real-time matching
 */
import React, { useState, useEffect } from 'react';
import { View, StyleSheet, Alert } from 'react-native';
import { SafeAreaView } from 'react-native-safe-area-context';
import { useNavigation } from '@react-navigation/native';
import { NativeStackNavigationProp } from '@react-navigation/native-stack';
import { RootStackParamList } from '@/presentation/navigation/RootNavigator';
import { COLORS, REALTIME_MATCHING_CONFIG } from '@/constants';
import { Text } from '@/presentation/components/common/Text';
import { ListenButton } from './components/ListenButton';
import { AudioRecorder } from '@/services/audio/AudioRecorder';
import { useRecognitionStore } from '@/presentation/store/recognitionStore';
import { useRecognition } from '@/presentation/hooks/useRecognition';
import { useRealTimeRecognition } from '@/presentation/hooks/useRealTimeRecognition';
import { ERROR_MESSAGES } from '@/constants/errors';
import { RecognitionResult } from '@/domain/entities/RecognitionResult';

type NavigationProp = NativeStackNavigationProp<RootStackParamList>;

export default function HomeScreen() {
  const navigation = useNavigation<NavigationProp>();
  const audioRecorder = useState(() => new AudioRecorder())[0];
  const { recognize, isProcessing } = useRecognition();
  const { isRecording, setRecording, setRecordingData, setError } =
    useRecognitionStore();

  // Real-time recognition hook
  const {
    isMatching,
    matchingConfidence,
    chunksSent,
    startMatching,
    stopMatching,
    processChunk,
    reset: resetRealTime,
  } = useRealTimeRecognition();

  // Set up auto-stop callback
  useEffect(() => {
    audioRecorder.setOnAutoStop(() => {
      stopRecording();
    });
  }, []);

  const handleButtonPress = async () => {
    if (isRecording) {
      // Stop recording
      await stopRecording();
    } else {
      // Start recording
      await startRecording();
    }
  };

  const startRecording = async () => {
    try {
      // Request permissions
      const hasPermission = await audioRecorder.requestPermissions();
      if (!hasPermission) {
        Alert.alert('Microphone Permission', ERROR_MESSAGES.MICROPHONE_DENIED, [
          { text: 'OK' },
        ]);
        return;
      }

      // Enable chunking with callback
      audioRecorder.enableChunking(
        { chunkInterval: REALTIME_MATCHING_CONFIG.CHUNK_INTERVAL },
        async (chunk) => {
          console.log(`ðŸ“¦ Chunk ${chunk.chunkIndex} ready:`, chunk.duration, 'ms');
          // Process chunk in real-time
          await processChunk(chunk);
        }
      );

      // Start real-time matching with auto-navigation callback
      startMatching((result: RecognitionResult) => {
        console.log('ðŸŽ¯ Match found! Auto-navigating...');

        // Auto-stop recording
        stopRecording();

        // Navigate directly to Surah with the matched verse
        if (result.surah && result.ayah) {
          navigation.navigate('Surah', {
            surahNumber: result.surah.number,
            surahName: result.surah.name,
            highlightAyah: result.ayah.number,
            fromRecognition: true,
          });
        }
      });

      // Start chunked recording
      await audioRecorder.startChunkedRecording();
      setRecording(true);

      console.log('ðŸŽ™ï¸ Started real-time matching...');
    } catch (error: any) {
      console.error('Start recording error:', error);
      Alert.alert('Error', 'Failed to start recording');
    }
  };

  const stopRecording = async () => {
    try {
      // Check if still recording before stopping
      if (!isRecording) {
        return;
      }

      setRecording(false);

      // Stop chunked recording
      const finalChunk = await audioRecorder.stopChunkedRecording();

      // Process final chunk if available
      if (finalChunk && isMatching) {
        await processChunk(finalChunk);
      }

      // Stop real-time matching
      stopMatching();

      // Disable chunking
      audioRecorder.disableChunking();

      console.log(`ðŸ›‘ Stopped recording. Total chunks sent: ${chunksSent}`);

      // If no match was found during real-time, show message
      if (matchingConfidence === 0) {
        Alert.alert(
          'No Match Found',
          'Could not identify the recitation. Please try again with a clearer audio.',
          [{ text: 'OK' }]
        );
      } else if (matchingConfidence < REALTIME_MATCHING_CONFIG.CONFIDENCE_THRESHOLD * 100) {
        Alert.alert(
          'Low Confidence',
          `Found a possible match but confidence was only ${matchingConfidence}%. Please try again.`,
          [{ text: 'OK' }]
        );
      }

      // Reset real-time state
      resetRealTime();
    } catch (error: any) {
      console.error('Stop recording error:', error);

      if (error.message === 'RECORDING_TOO_SHORT') {
        Alert.alert('Too Short', ERROR_MESSAGES.AUDIO_TOO_SHORT);
      } else if (error.message === 'RECORDING_TOO_LONG') {
        Alert.alert('Too Long', 'Recording exceeded maximum duration. Please try again.');
      } else {
        Alert.alert('Error', 'Failed to process recording');
      }

      setError(error.message);
      resetRealTime();
    }
  };

  return (
    <SafeAreaView style={styles.container} edges={['top']}>
      <View style={styles.content}>
        {/* Header */}
        <View style={styles.header}>
          <Text variant="h1" align="center">
            Ayahfinder
          </Text>
          <Text variant="body" color={COLORS.text.secondary} align="center">
            Identify Quran recitations instantly
          </Text>
        </View>

        {/* Listen Button */}
        <View style={styles.buttonContainer}>
          <ListenButton
            isRecording={isRecording}
            isProcessing={isProcessing}
            onPress={handleButtonPress}
          />
        </View>

        {/* Real-time Matching Feedback */}
        {isMatching && (
          <View style={styles.matchingFeedback}>
            <Text variant="body" align="center" color={COLORS.accent.default}>
              ðŸŽ§ Listening...
            </Text>
            {matchingConfidence > 0 && (
              <>
                <View style={styles.confidenceBar}>
                  <View
                    style={[
                      styles.confidenceFill,
                      { width: `${matchingConfidence}%` },
                    ]}
                  />
                </View>
                <Text variant="caption" align="center" color={COLORS.text.primary}>
                  Matching: {matchingConfidence}%
                </Text>
              </>
            )}
            <Text variant="caption" align="center" color={COLORS.text.secondary}>
              Chunks sent: {chunksSent}
            </Text>
          </View>
        )}

        {/* Instructions */}
        {!isMatching && (
          <View style={styles.instructions}>
            <Text variant="caption" align="center" color={COLORS.text.secondary}>
              Tap the button and play a Quran recitation
            </Text>
            <Text variant="caption" align="center" color={COLORS.text.secondary}>
              It will automatically find and show the verse
            </Text>
          </View>
        )}
      </View>
    </SafeAreaView>
  );
}

const styles = StyleSheet.create({
  container: {
    flex: 1,
    backgroundColor: COLORS.background.default,
  },
  content: {
    flex: 1,
    justifyContent: 'space-between',
    paddingVertical: 48,
  },
  header: {
    paddingHorizontal: 24,
    gap: 8,
  },
  buttonContainer: {
    flex: 1,
    justifyContent: 'center',
    alignItems: 'center',
  },
  instructions: {
    paddingHorizontal: 24,
    gap: 4,
  },
  matchingFeedback: {
    paddingHorizontal: 24,
    gap: 12,
    alignItems: 'center',
  },
  confidenceBar: {
    width: '100%',
    height: 8,
    backgroundColor: COLORS.background.tertiary,
    borderRadius: 4,
    overflow: 'hidden',
  },
  confidenceFill: {
    height: '100%',
    backgroundColor: COLORS.accent.default,
    borderRadius: 4,
  },
});
